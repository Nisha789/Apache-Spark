# Apache-Spark: PySpark Hands-On Exercises

Welcome to the **Apache-Spark** repository! This repository contains hands-on exercises in **PySpark**, designed to help you learn and explore the core functionalities of **Apache Spark**. All exercises are crafted to be run on **Google Colab**, making it easy to dive into Spark without extensive setup.

## üìù Table of Contents
- [About the Project](#about-the-project)
- [Getting Started](#getting-started)
- [Exercises](#exercises)
- [Setup in Google Colab](#setup-in-google-colab)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

## üìå About the Project
This repository includes various PySpark exercises that cover topics such as:
- Data ingestion and processing
- Transformations and actions
- Spark SQL and DataFrames
- Aggregations and joins
- Performance tuning

These exercises are tailored to provide hands-on practice with real-world scenarios, enhancing your PySpark skills in a cloud-based environment.

## üöÄ Getting Started

To get started, open the exercises in Google Colab. You can run all the exercises directly in Colab without any local setup.

## üìÇ Exercises

The repository currently includes the following PySpark notebooks:

| Notebook                     | Description                                                                                  |
|------------------------------|----------------------------------------------------------------------------------------------|
| `Joins.ipynb`                | Hands-on exercises focusing on different types of joins in PySpark, including inner, outer, and cross joins. | 
| `PySpark_Transformations.ipynb` | Exercises covering core PySpark transformations, including filtering, mapping, and aggregations. |
| `Spark_Data_Sources.ipynb`   | Examples of working with various data sources in PySpark, such as CSV, JSON, and Parquet files. | 

These notebooks are organized to help you understand essential PySpark operations and are ready to run in Google Colab for ease of use.

## ‚öôÔ∏è Setup in Google Colab

Since Google Colab does not come with PySpark pre-installed, you can install it by adding the following code at the beginning of each notebook: `!pip install pyspark`
Additionally, for any specific Spark configurations, you can set them up as needed within each notebook.

## üéØ Usage
- Open the notebook file on Google Colab.
- Run the setup cell to install PySpark: `!pip install pyspark`
Follow the instructions within each notebook to complete the exercises.

## ü§ù Contributing
Contributions are welcome! If you have new exercises, bug fixes, or improvements, feel free to fork this repository and submit a pull request.

## üìú License
This project is licensed under the MIT License. See the LICENSE file for details.